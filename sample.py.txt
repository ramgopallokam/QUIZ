Video Link:

https://drive.google.com/file/d/1snCEaplvf7GmA7esVwlfjCwXatex6-_l/view

1)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Load dataset
data = pd.read_csv('temperature_data.csv')

# Convert 'Date' column to datetime
data['Date'] = pd.to_datetime(data['Date'])

# Sort by date
data.sort_values('Date', inplace=True)

# Normalize the temperature data
scaler = MinMaxScaler(feature_range=(0, 1))
data['Temperature'] = scaler.fit_transform(data['Temperature'].values.reshape(-1, 1))

# Split dataset into training and testing sets (e.g., 80-20 split)
train_size = int(len(data) * 0.8)
train_data, test_data = data[:train_size], data[train_size:]

# Optionally, you can reset index for both train_data and test_data
train_data.reset_index(drop=True, inplace=True)
test_data.reset_index(drop=True, inplace=True)

# Save preprocessed data if needed
train_data.to_csv('train_data.csv', index=False)
test_data.to_csv('test_data.csv', index=False)

2)

from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense

# Define the number of units in each LSTM layer
units = [64, 64]  # Example: Two LSTM layers with 64 units each

# Initialize the model
model = Sequential()

# Add the first LSTM layer with dropout
model.add(LSTM(units[0], return_sequences=True, input_shape=(n_timesteps, n_features)))
model.add(Dropout(0.2))  # Adding dropout to prevent overfitting

# Add additional LSTM layers with dropout (if any)
for i in range(1, len(units)):
    model.add(LSTM(units[i], return_sequences=True))
    model.add(Dropout(0.2))  # Adding dropout to prevent overfitting

# Add a final dense layer for output
model.add(Dense(1))  # Output layer with one neuron for regression tasks

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')


3)


# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))

# Plot training/validation loss
import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


4)

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)

# Visualize predictions vs ground truth
plt.figure(figsize=(10, 6))
plt.plot(y_test, label='Ground Truth', color='blue')
plt.plot(y_pred, label='Predictions', color='red')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('LSTM Model Predictions vs Ground Truth')
plt.legend()
plt.show()


5)


from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import GridSearchCV
from keras.optimizers import Adam

# Define a function to create the LSTM model
def create_model(learning_rate=0.001, units=50, dropout_rate=0.2):
    model = Sequential()
    model.add(LSTM(units, return_sequences=True, input_shape=(n_timesteps, n_features)))
    model.add(Dropout(dropout_rate))
    model.add(LSTM(units, return_sequences=True))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    
    optimizer = Adam(lr=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

# Create KerasRegressor wrapper
model = KerasRegressor(build_fn=create_model, verbose=0)

# Define hyperparameters grid
param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'units': [50, 100],
    'dropout_rate': [0.2, 0.3],
    'batch_size': [16, 32, 64]
}

# Perform grid search
grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
grid_result = grid.fit(X_train, y_train)

# Print best hyperparameters
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
